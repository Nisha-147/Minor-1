# ML Model for Blind Assistive System ğŸ™ï¸ğŸ‘ï¸

An endâ€‘toâ€‘end machine learning pipeline designed to support accessibility by enabling **voiceâ€‘toâ€‘text transcription** and **image/audio classification** for blind and visually impaired users. This project integrates **Whisper** (for speech recognition) and **PyTorch** (for vision models), with **FFmpeg** as the audio backend.

## âœ¨ Features
- Voiceâ€‘toâ€‘Text transcription using [OpenAI Whisper](https://github.com/openai/whisper)
- Image classification with a ResNetâ€‘based PyTorch model
- Textâ€‘toâ€‘Speech feedback (via `pyttsx3`) for blindâ€‘assistive interaction
- Modular training pipeline with train/val/test splits
- Easy deployment on Windows, macOS, or Linux

## ğŸ“¦ Installation
### Prerequisites
- Python 3.9+
- Git
- FFmpeg (required for Whisper)

### Install FFmpeg
**Windows (Chocolatey):**
```powershell
choco install ffmpeg
